{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560c5c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import all libraries\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib  # Using joblib instead of pickle, it's more efficient for scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Import the imblearn pipeline and SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04999a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Shape: (4999, 21)\n",
      "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
      "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
      "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
      "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
      "3  7795-CFOCW    Male              0      No         No      45           No   \n",
      "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
      "\n",
      "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
      "0  No phone service             DSL             No  ...               No   \n",
      "1                No             DSL            Yes  ...              Yes   \n",
      "2                No             DSL            Yes  ...               No   \n",
      "3  No phone service             DSL            Yes  ...              Yes   \n",
      "4                No     Fiber optic             No  ...               No   \n",
      "\n",
      "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
      "0          No          No              No  Month-to-month              Yes   \n",
      "1          No          No              No        One year               No   \n",
      "2          No          No              No  Month-to-month              Yes   \n",
      "3         Yes          No              No        One year               No   \n",
      "4          No          No              No  Month-to-month              Yes   \n",
      "\n",
      "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
      "0           Electronic check          29.85         29.85    No  \n",
      "1               Mailed check          56.95        1889.5    No  \n",
      "2               Mailed check          53.85        108.15   Yes  \n",
      "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
      "4           Electronic check          70.70        151.65   Yes  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "#Cell 2: Load the \"v1\" Data\n",
    "#\n",
    "# --- IMPORTANT ---\n",
    "# Change this path to point to where you saved your 5,000-row file\n",
    "DATA_V1_PATH = r\"C:\\Users\\fandi\\Downloads\\churn_data_v1.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(DATA_V1_PATH)\n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find file at {DATA_V1_PATH}\")\n",
    "    print(\"Please update the DATA_V1_PATH variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db28d9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting preprocessing...\n",
      "Filled 'TotalCharges' NaNs with median: 1397.65\n",
      "Dropped 'customerID' column.\n",
      "Mapped 'Churn' from string ('Yes'/'No') to 1/0.\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Basic Preprocessing & Cleaning (NEW ROBUST VERSION)\n",
    "#\n",
    "print(\"\\nStarting preprocessing...\")\n",
    "\n",
    "# 1. Fix TotalCharges: convert to numeric and fill missing\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "median_charges = df['TotalCharges'].median()\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(median_charges) # Safer syntax\n",
    "print(f\"Filled 'TotalCharges' NaNs with median: {median_charges}\")\n",
    "\n",
    "# 2. Drop the customerID column (it's not a feature)\n",
    "if 'customerID' in df.columns:\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "    print(\"Dropped 'customerID' column.\")\n",
    "\n",
    "# 3. *** NEW ROBUST MAPPING ***\n",
    "#    This checks if the column is *already* numbers. If it is, it skips the map.\n",
    "#    This prevents the \"re-run\" error!\n",
    "if pd.api.types.is_string_dtype(df['Churn']):\n",
    "    df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "    print(\"Mapped 'Churn' from string ('Yes'/'No') to 1/0.\")\n",
    "else:\n",
    "    print(\"'Churn' column is already numeric, mapping skipped.\")\n",
    "\n",
    "# 4. Handle potential missing target values\n",
    "#    This was our original 'dirty data' fix. It's still good to keep.\n",
    "initial_rows = len(df)\n",
    "df.dropna(subset=['Churn'], inplace=True)\n",
    "final_rows = len(df)\n",
    "if initial_rows > final_rows:\n",
    "    print(f\"Dropped {initial_rows - final_rows} rows with missing 'Churn' (target) values.\")\n",
    "\n",
    "# 5. Convert Churn to integer (it's now 0.0 or 1.0)\n",
    "df['Churn'] = df['Churn'].astype(int)\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e9eac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features (X) shape: (4999, 19)\n",
      "Target (y) shape: (4999,)\n",
      "Class distribution:\n",
      "Churn\n",
      "0    0.737347\n",
      "1    0.262653\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define Features (X) and Target (y)\n",
    "#\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb2a26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building preprocessing pipeline...\n",
      "Preprocessor created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Build the Preprocessing Pipeline\n",
    "#\n",
    "print(\"\\nBuilding preprocessing pipeline...\")\n",
    "\n",
    "# Define which columns are which type\n",
    "# Note: SeniorCitizen is 0/1 but we'll treat it as categorical for one-hot encoding\n",
    "categorical_features = [\n",
    "    'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', \n",
    "    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', \n",
    "    'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
    "]\n",
    "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Pass through any columns not listed\n",
    ")\n",
    "\n",
    "print(\"Preprocessor created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e0b7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape: (3999, 19)\n",
      "Test set shape: (1000, 19)\n",
      "Test set class distribution:\n",
      "Churn\n",
      "0    0.737\n",
      "1    0.263\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Split Data into Train and Test Sets\n",
    "#\n",
    "# We use stratify=y to ensure the test set has the same class imbalance as the full dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Test set class distribution:\\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ee8502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating full model pipeline with SMOTE...\n",
      "Pipeline created successfully.\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('num', StandardScaler(),\n",
      "                                                  ['tenure', 'MonthlyCharges',\n",
      "                                                   'TotalCharges']),\n",
      "                                                 ('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False),\n",
      "                                                  ['gender', 'SeniorCitizen',\n",
      "                                                   'Partner', 'Dependents',\n",
      "                                                   'PhoneService',\n",
      "                                                   'MultipleLines',\n",
      "                                                   'InternetService',\n",
      "                                                   'OnlineSecurity',\n",
      "                                                   'OnlineBackup',\n",
      "                                                   'DeviceProtection',\n",
      "                                                   'TechSupport', 'StreamingTV',\n",
      "                                                   'StreamingMovies',\n",
      "                                                   'Contract',\n",
      "                                                   'PaperlessBilling',\n",
      "                                                   'PaymentMethod'])])),\n",
      "                ('smote', SMOTE(random_state=42)),\n",
      "                ('classifier', RandomForestClassifier(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Create the Full Model Pipeline (Handling Imbalance)\n",
    "#\n",
    "print(\"\\nCreating full model pipeline with SMOTE...\")\n",
    "\n",
    "# We use the special Pipeline from imbalanced-learn\n",
    "# This pipeline correctly applies SMOTE only to the training data during .fit()\n",
    "model_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_estimators=100))\n",
    "])\n",
    "\n",
    "print(\"Pipeline created successfully.\")\n",
    "print(model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa5f6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the baseline model (v1)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the baseline model (v1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fandi\\miniconda3\\envs\\churn_mlops\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the baseline model (v1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fandi\\miniconda3\\envs\\churn_mlops\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train the Model\n",
    "#\n",
    "print(\"\\nTraining the baseline model (v1)...\")\n",
    "# This one command runs all steps: preprocess, smote, train\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c795e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating baseline model on the test set...\n",
      "*********************************************\n",
      "** Baseline Model F1-Score (Class 1 'Churn'): 0.6000 **\n",
      "*********************************************\n",
      "\n",
      "Full Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Churn (0)       0.86      0.84      0.85       737\n",
      "   Churn (1)       0.58      0.62      0.60       263\n",
      "\n",
      "    accuracy                           0.78      1000\n",
      "   macro avg       0.72      0.73      0.73      1000\n",
      "weighted avg       0.79      0.78      0.79      1000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[622 115]\n",
      " [101 162]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Evaluate the \"v1\" Model (Handling Imbalance)\n",
    "#\n",
    "print(\"\\nEvaluating baseline model on the test set...\")\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# --- KEY METRIC ---\n",
    "# We focus on F1-score for the positive (Churn) class, not overall accuracy\n",
    "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "print(f\"*********************************************\")\n",
    "print(f\"** Baseline Model F1-Score (Class 1 'Churn'): {f1:.4f} **\")\n",
    "print(f\"*********************************************\")\n",
    "\n",
    "print(\"\\nFull Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn (0)', 'Churn (1)']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "299f0bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Manual Bias Check (SeniorCitizen) ---\n",
      "F1-Score for Seniors (1): 0.6232 (on 154 samples)\n",
      "F1-Score for Non-Seniors (0): 0.5920 (on 846 samples)\n",
      "\n",
      "Note: A large difference in F1-scores indicates potential bias.\n",
      "We will automate this check in our MLOps pipeline.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Analyze Bias (Handling Bias)\n",
    "#\n",
    "print(\"\\n--- Manual Bias Check (SeniorCitizen) ---\")\n",
    "# We test our pipeline's performance on a specific slice of the test data\n",
    "# The model_pipeline.predict() command will correctly preprocess this slice\n",
    "try:\n",
    "    # Slice 1: Senior Citizens\n",
    "    X_test_seniors = X_test[X_test['SeniorCitizen'] == 1]\n",
    "    y_test_seniors = y_test.loc[X_test_seniors.index]\n",
    "    y_pred_seniors = model_pipeline.predict(X_test_seniors)\n",
    "    f1_seniors = f1_score(y_test_seniors, y_pred_seniors, pos_label=1)\n",
    "    print(f\"F1-Score for Seniors (1): {f1_seniors:.4f} (on {len(y_test_seniors)} samples)\")\n",
    "\n",
    "    # Slice 2: Non-Senior Citizens\n",
    "    X_test_non_seniors = X_test[X_test['SeniorCitizen'] == 0]\n",
    "    y_test_non_seniors = y_test.loc[X_test_non_seniors.index]\n",
    "    y_pred_non_seniors = model_pipeline.predict(X_test_non_seniors)\n",
    "    f1_non_seniors = f1_score(y_test_non_seniors, y_pred_non_seniors, pos_label=1)\n",
    "    print(f\"F1-Score for Non-Seniors (0): {f1_non_seniors:.4f} (on {len(y_test_non_seniors)} samples)\")\n",
    "    \n",
    "    print(\"\\nNote: A large difference in F1-scores indicates potential bias.\")\n",
    "    print(\"We will automate this check in our MLOps pipeline.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not perform bias check: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd5fcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model pipeline to model_v1.pkl...\n",
      "Model saved successfully to model_v1.pkl.\n",
      "\n",
      "--- STEP 1 COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Save Your Pipeline Object\n",
    "#\n",
    "MODEL_SAVE_PATH = \"model_v1.pkl\"\n",
    "print(f\"\\nSaving model pipeline to {MODEL_SAVE_PATH}...\")\n",
    "\n",
    "# Use joblib.dump to save the entire pipeline (preprocessor + smote + model)\n",
    "joblib.dump(model_pipeline, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved successfully to {MODEL_SAVE_PATH}.\")\n",
    "print(\"\\n--- STEP 1 COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49f72cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m  \u001b[38;5;66;03m# To get the TARGET_COLUMN name\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;66;03m# Required for the is_string_dtype check\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# --- RE-LOAD AND RE-SPLIT DATA ---\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# NOTE: This MUST use the same random_state=42 and stratify=y as before \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# to generate the exact same test set.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Assuming your original v1 data file is still accessible.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scripts.utils'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# FIX: Add the project root to sys.path so it can find scripts/utils.py\n",
    "if '.' not in sys.path:\n",
    "    sys.path.append('.')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scripts.utils as utils  # To get the TARGET_COLUMN name\n",
    "import pandas.api.types # Required for the is_string_dtype check\n",
    "\n",
    "# --- RE-LOAD AND RE-SPLIT DATA ---\n",
    "# NOTE: This MUST use the same random_state=42 and stratify=y as before \n",
    "# to generate the exact same test set.\n",
    "\n",
    "# Assuming your original v1 data file is still accessible.\n",
    "df = pd.read_csv(\"churn_data_v1.csv\")\n",
    "\n",
    "# Perform the same preprocessing to clean TotalCharges before splitting\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "median_charges = df['TotalCharges'].median()\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(median_charges)\n",
    "if 'customerID' in df.columns:\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "# Map the target variable 'Churn' to 0 and 1 (robust fix)\n",
    "if pd.api.types.is_string_dtype(df[utils.TARGET_COLUMN]):\n",
    "    df[utils.TARGET_COLUMN] = df[utils.TARGET_COLUMN].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "df.dropna(subset=[utils.TARGET_COLUMN], inplace=True)\n",
    "df[utils.TARGET_COLUMN] = df[utils.TARGET_COLUMN].astype(int)\n",
    "\n",
    "# --- Define X and y (Crucial for stratification) ---\n",
    "X = df.drop(utils.TARGET_COLUMN, axis=1)\n",
    "y = df[utils.TARGET_COLUMN]\n",
    "\n",
    "\n",
    "# --- Re-split to grab the X_test and y_test sets ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Create the Golden Test Set File ---\n",
    "# Combine X_test and y_test back into one DataFrame\n",
    "test_set_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file in your project root\n",
    "test_set_df.to_csv(\"test_set.csv\", index=False)\n",
    "\n",
    "print(f\"Golden Test Set saved successfully! Shape: {test_set_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
